{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<div class=\"alert alert-info\"> <h1> Spam / Ham Detection using XGBoost",
   "id": "87f975adffbb3563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "from sympy.abc import alpha\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "d = pd.read_csv('fraud_detect.csv')\n",
    "data = d.copy()\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.drop(df[df['Category'] == '{\"mode\":\"full\"'].index, axis=0)\n",
    "df = df.drop_duplicates(subset=['Message'])\n",
    "\n"
   ],
   "id": "d82b084cc5bfb9b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-info\"> <h2> Clean Text Data:</h2>\n",
    "<p> Clean text data by removing stopwords, lemmatizing, unnecessary characters, punctuation and tokenize the sentences"
   ],
   "id": "d731d30d7189845e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # remove links, url, www etc\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    # remove email etc\n",
    "    text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"\", text)\n",
    "\n",
    "\n",
    "    text = text.lower()\n",
    "    return text"
   ],
   "id": "66d8ee965f8ef16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['Message'] = df['Message'].apply(clean_text)\n",
    "df['Category_Number'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ],
   "id": "c51297293010d5f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-info\"> <h2> Begin Exploratory Data Analysis</h2>\n",
    "<h3> WordCloud"
   ],
   "id": "8b30c0969d5deabc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "colors = sns.color_palette(\"Set2\", 4)\n",
    "colors"
   ],
   "id": "90ed137a874d10a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ham_messages = df[df['Category'] == 'ham']['Message']\n",
    "spam_messages = df[df['Category'] == 'spam']['Message']\n",
    "\n",
    "ham_text = ' '.join(ham_messages)\n",
    "spam_text = ' '.join(spam_messages)\n",
    "\n",
    "ham_wordcloud = WordCloud(\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    background_color='black',\n",
    "    max_words=200,\n",
    "    colormap='Blues'\n",
    "\n",
    ").generate(ham_text)\n",
    "\n",
    "spam_wordcloud = WordCloud(\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    background_color='black',\n",
    "    max_words=200,\n",
    "    colormap='Reds'\n",
    "\n",
    ").generate(ham_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(ham_wordcloud, interpolation='bilinear')\n",
    "plt.title('ham')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(spam_wordcloud, interpolation='bilinear')\n",
    "plt.title('spam')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9d7b7fd19c36773f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "category_counts = df['Category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = sns.color_palette(\"Set2\", 2)\n",
    "plt.pie(category_counts, labels=['Ham', 'Spam'], colors=colors, autopct='%1.0f%%')\n",
    "plt.title('Categories Distribution')\n",
    "plt.show()"
   ],
   "id": "ec6c134e5c66df81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<div class=\"alert alert-info\"> <h2> Analyze the Message Length and add col to the DataFrame",
   "id": "60dbfec2a99fecdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['Message_Length'] = df['Message'].str.len()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=df, x='Message_Length', hue='Category', kde=True, palette='viridis', bins=30)\n",
    "plt.title('Message Length Distribution')\n",
    "plt.xlabel('Message Length')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ],
   "id": "bef185a3d70927d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f73422d90687aae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-info\"> <h3> <i> Result:</i><br>Messages contain < 200 characters</h3>\n",
    "<h2> Analyze Most Frequent Words: <b>Unigrams, Bigrams, Trigrams</b></h2>\n",
    "<p> Erklärung: Sie helfen bei der Erstellung von Modellen, die die Wahrscheinlichkeit von Wortfolgen in einem Text abschätzen.\n",
    "Beispiel: In einem Bigram-Modell könnte die Wahrscheinlichkeit von \"ist ein\" berechnet werden, basierend darauf, wie oft diese Kombination in einem Trainingsdatensatz vorkommt."
   ],
   "id": "86fed4a72197a632"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_unigrams(category):\n",
    "    words = ''.join(df[df['Category'] == category]['Message'])\n",
    "    word_tokens = nltk.word_tokenize(words.lower())\n",
    "    word_counts = Counter(word_tokens)\n",
    "    most_common_words = word_counts.most_common(20)\n",
    "\n",
    "    words, counts = zip(*most_common_words)\n",
    "\n",
    "\n",
    "def get_bigrams(category):\n",
    "    vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "    ngrams = vectorizer.fit_transform(df[df['Category'] == category]['Message']).toarray()\n",
    "    sum_ngrams = ngrams.sum(axis=0).flatten()\n",
    "\n",
    "    bigram_freq = dict(zip(vectorizer.get_feature_names_out(), sum_ngrams))\n",
    "    sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    bigrams, freqs = zip(*sorted_bigrams[:20])\n",
    "\n",
    "\n",
    "def get_trigrams(category):\n",
    "    vectorizer = CountVectorizer(ngram_range=(3, 3), stop_words='english')\n",
    "    ngrams = vectorizer.fit_transform(df[df['Category'] == category]['Message']).toarray()\n",
    "    sum_ngrams = ngrams.sum(axis=0).flatten()\n",
    "    trigram_freq = dict(zip(vectorizer.get_feature_names_out(), sum_ngrams))\n",
    "    sorted_trigrams = sorted(trigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    trigrams, freqs = zip(*sorted_trigrams[:20])\n"
   ],
   "id": "9e5a63353610c662"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<div class=\"alert alert-info\"> <h2> Prepare Dataset for Model training",
   "id": "8b8b9123ddf3f3ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = df['Message']\n",
    "y = df['Category_Number']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ],
   "id": "8b1a277a727870ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<div class=\"alert alert-success\"> <h2> Find optimal Model Parameters:",
   "id": "108c1a09dc0d8523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_params = {'n_estimators': 467,\n",
    " 'max_depth': 4,\n",
    " 'learning_rate': 0.14014899942971895,\n",
    " 'subsample': 0.9968781941622266,\n",
    " 'colsample_bytree': 0.6812355146666758,\n",
    " 'gamma': 0.46844534449232467,\n",
    " 'min_child_weight': 1}"
   ],
   "id": "95343cc362234d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = XGBClassifier(**best_params)\n",
    "model.fit(X_train_tfidf, y_train)"
   ],
   "id": "f88ac24a75b1ac85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "class_names = ['ham', 'spam']\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ],
   "id": "d808a9c78592d628"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.show()"
   ],
   "id": "8fd06bf3d62ded9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<div class=\"alert alert-info\"> <h2> Test Model",
   "id": "b11f1c10f5a99bdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "new = \"Congratulations! You've won a free iPhone 15! To claim your prize, just follow this link and fill out the form: [link]. Don't miss out, offer ends soon!\"\n",
    "\n",
    "\n",
    "def spam_check(text):\n",
    "    tfidf = vectorizer.transform([text])\n",
    "    predictions = model.predict(tfidf)\n",
    "    confidences = model.predict_proba(tfidf)\n",
    "    label_mapping = {1: 'SPAM', 0: 'HAM'}\n",
    "    result = predictions[0]\n",
    "    confidence = np.round(confidences[0][result] * 100, 4)\n",
    "    return label_mapping[result], confidence\n",
    "\n",
    "\n",
    "\n",
    "res, conf = spam_check(new)\n",
    "print(res, conf)"
   ],
   "id": "31de01f28dbda6bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
